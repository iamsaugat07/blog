[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "$hello_world!",
    "section": "",
    "text": "My curriculum for learning machine learning in 2023\n\n\n\n\n\n\nMachine learning & AI\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSaugat\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nSaugat\n\n\n\n\n\n\n\n\n\n\n\n\nHeart disease prediction\n\n\n\n\n\n\nMachine learning & AI\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\nSaugat\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mlcurriculum/index.html",
    "href": "posts/mlcurriculum/index.html",
    "title": "My curriculum for learning machine learning in 2023",
    "section": "",
    "text": "(env) saugat: brain/cerebral cortex/machine learning-AI\nDisclaimer : This post is not written or edited by chatgpt or any AI tool üòÅ\nPheww! The world has seen the tremendous impact of large language models(LLMs) like gpt-3.5 or gpt 4 in recent times. There has been new emerging fields like prompt engineering, LLMOps which was never seen before. The foundation for all these emerging technology has always been machine learning and software engineering. To be more specific Natural language processing and its applications augmented by different software infrastructure and data.\nThe topic of LLMs can be a different blog post. Lets get back to our main topic.\nHere is little background: I am mechanical engineering graduate of 2022 from Institute of engineering(IOE), TU Nepal. Having worked as design and CFD engineer with couple of startups in automotive industry in Nepal for almost 2 years, (I even started racing team in my college: Team Shireto) I decided to switch to world of machine learning and AI in march of 2023. Learning new things is always exciting for me. One of the habits or instinct that you need to learn when you come from engineering background is learning the hard concepts quickly and being able to create something valuable out of it aka solving an engineering problem. The vast application of machine learning in different industry ranging from automotive to finance to healthcare and so many more is one of the reason behind my focus on machine learning to solve engineering problems. You are not limited to a certain domain when it comes to applying machine learning in contrast with the field I was previously focusing on.\nSo how did I get started with learning machine learning in 2023?\nI went through internet and found tons of free resources. As I couldn‚Äôt afford to pay for most of the online courses these free resources are like diamonds for me. Damn! we live in a world where we can learn anything for free. All those readings boiled down to this curriculum which I created for myself to become a self taught ML engineer.\nOne of the main challenge to get started with machine learning was programming. Maths was not a big problem as my engineering degree covered all the required maths foundation for machine learning.\nLet me list down courses in a format that I am following during this journey of becoming an ML practitioner.\n\nComplete Machine Learning & Data Science Bootcamp 2023 by ZTM\n\nWhy this course?\nHmm‚Ä¶ So this is the only course that I paid for. Learning something new comes with lot of resistance in the beginning. I have fell into enroll and never see the course again loop so many times before when I wanted to learn programming, therefore I decided to pay for this course so I would push myself to learn consistently and finish the course start to end, which I eventually did in almost 2 months. This course help me build my foundation on programming as it focuses on run the code approach and project based approach to learn programming.\nI learnt about python as required for me to get started for using specialized python based packages for ML & data science like Numpy, pandas, matplotlib and scikitlearn. During the course I build three projects which I will write about in my future blog. In summary, I worked on heart disease classification project, bulldozer auction price prediction project and dog breed identification project(using tensorflow). To be honest most of the things went over my head while working on these projects. But I worked hard and made sure I finish the course no matter what. I watched videos on loop, searched internet, interacted with community built around the course and asked ChatGPT to digest the content and ingest the concept into my memory.\n\nCS50 Harvard‚Äôs Introduction to computer science\n\nTo understand more about computer science and programming in general, I joined this course on free version. I haven‚Äôt finished it yet. As the course problem set are quite challenging and takes time. I am huge fan of interleaving, which basically means interleaving between concepts in different domains or subject to learn proactively. So during my boot camp, I interleaved between courses to learn effectively. But how? I will write a another blog post on this üòÇ.\nThis course focuses on fundamentals of computer science from binary to memory, from arrays to algorithms and different programming language like python, C, HTML, CSS, Javascript & SQL. Hopefully I will be able to finish this course very soon.\n\nMachine learning specialization by Deeplearning.ai\n\nThis has to be the best among all the course in my list. Wait not only on my list but whole courses on ML or engineering in the whole world. I became a huge fan of Andrew when I joined this course. The way he explained the concepts from basics and visualize all the fundamental concepts like gradient descent, cost function, evaluation metrics, neural network, Random forest from scratch is extraordinary. The code implementation of ML algorithms from scratch was the best part of this course. Talking about interleaving, while working on my classification problem during boot camp I joined Andrew‚Äôs course to understand the underlying concepts by watching the videos on logistic regression and classification. Still I haven‚Äôt finished all the courses in this specialization. But whatever I have learned so far has been so fruitful to build my intuition for machine learning.\n\nFast.ai deep learning for coders\n\nThis is the toughest course among all for me. Part of the reason is the book that I am following alongside the course. The book and the course implements top to bottom approach when it comes to learning, which is unusual and unique when comparing to all other courses. What does that mean? Basically top to bottom approach is to start learning by seeing the application of the code and getting your model running before actually going deep inside the theory and the behind the scenes of the algorithms, which is completely opposite to way it is taught the machine learning specialization. There are two part in this course. One is deep learning foundations another is deep learning foundations to stable diffusion.\nI am currently on part one where I have finished first four lesson where I learned about getting started with neural network, getting your model to production aka building ml based web app and basic NLP.\n\nFull Stack deep learning\n\nThis has to be bring it all together course where you will not learn how to write your own model or creating training and validation sets but you will learn how to build products powered by ML. One of the main concept or eye opener for me during this course was that the training of models is starting to become commoditized. I learnt about different tools like huggingface that you can use to deploy state-of-the-art models in few lines of code. This is the most exciting course in the list where you focus on learning ML product engineering.\n\n\nChatGPT\n\nWell ChatGPT is more off a tool to learn and search about some topics and get a quick read rather than searching the internet. During my interaction with ChatGPT, I am focusing to write good prompts so that I can get what I want from the GPT3.5 model. It helps with quick example of code and ask MCQ related to the explanation. \n\n\n\n\nThe missing semester of your CS education\n\nThis course teaches about computer ecosystem literacy aka the tools you need to use during your career as a software engineer or developer. It has all the necessary topic that you will encounter during the journey of becoming a developer and helps with saving time using the tools and concepts like shell, command line interface, data wrangling, version control, debugging & profiling and more.\nAlongside following the course I think it is also important to look around the community and follow top ML practitioner who share their knowledge and experiences through blogs, podcast and tweets.\nHere is the list of ML practitioner who have influenced me a lot (list of practitioner who are actively writing, tweeting & sharing their learnings)\n\nAndrej Karpathy\nChip Huyen\nEmil Walner\nJeremy Howard\nJonas Degrave\n\nAnd of course Andrew Ng is my favorite üëç\nI feel like each subtopic in this blog can be converted into another blog post and that blog post‚Äôs subtopic into another in form of nested blogs. In future, I will come up with more blogs sharing about my learnings, practices and mistakes along the way. I would love to hear from other people who might read my blog and share their feedback on how can I learn better and get better in Machine learning & AI. Now, alongside these course I will start building my own personal projects, contributing to opensource and eventually developing & deploying ML(or LLMsüòâ) powered products.\nEnding this blog with a tweet from Andrej."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html",
    "title": "Heart disease prediction",
    "section": "",
    "text": "This notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether someone has heart disease based on their medical attributes\nwe‚Äôre going to take following approach: 1. Problem definition 2. Data 3. Evaluation 4. Features 5. Modeling 6. Experimentation"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#problem-definition",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#problem-definition",
    "title": "Heart disease prediction",
    "section": "1. Problem definition",
    "text": "1. Problem definition\nIn a statement, &gt; Given clinical parameters about a patient, can we predict a patient has heart disease."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#data",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#data",
    "title": "Heart disease prediction",
    "section": "2. Data",
    "text": "2. Data\nThe original data came from the Cleavland data from UCI machine learning repository - https://archive.ics.uci.edu/ml/datasets/heart+disease\nOn kaggle - https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluation",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluation",
    "title": "Heart disease prediction",
    "section": "3. Evaluation",
    "text": "3. Evaluation\n\nIf we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we‚Äôll pursue the project."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#features",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#features",
    "title": "Heart disease prediction",
    "section": "4. Features",
    "text": "4. Features\nThis is where you‚Äôll get different information about each of the features in your data\ncreate data dictionary\n\nage - age in years\nsex - (1 = male; 0 = female)\ncp - chest pain type\n\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related) *3: Asymptomatic: chest pain not showing signs of disease\n\ntrestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\nanything above 130-140 is typically cause for concern\n\nchol - serum cholestoral in mg/dl\n\nserum = LDL + HDL + .2 * triglycerides\nabove 200 is cause for concern\n\nfbs - (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n\n‚Äò&gt;126‚Äô mg/dL signals diabetes\n\nrestecg - resting electrocardiographic results\n\n0: Nothing to note\n1: ST-T Wave abnormality\n\ncan range from mild symptoms to severe problems\nsignals non-normal heart beat\n\n2: Possible or definite left ventricular hypertrophy\n\nEnlarged heart‚Äôs main pumping chamber\n\n\nthalach - maximum heart rate achieved\nexang - exercise induced angina (1 = yes; 0 = no)\noldpeak - ST depression induced by exercise relative to rest\n\nlooks at stress of heart during excercise\nunhealthy heart will stress more\n\nslope - the slope of the peak exercise ST segment\n\n0: Upsloping: better heart rate with excercise (uncommon)\n1: Flatsloping: minimal change (typical healthy heart)\n2: Downslopins: signs of unhealthy heart\n\nca - number of major vessels (0-3) colored by flourosopy\n\ncolored vessel means the doctor can see the blood passing through\n\nthal - thalium stress result\n\n1,3: normal\n6: fixed defect: used to be defect but ok now\n7: reversable defect: no proper blood movement when excercising\n\ntarget - have disease or not (1=yes, 0=no) (= the predicted attribute)"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#preparing-the-tools",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#preparing-the-tools",
    "title": "Heart disease prediction",
    "section": "Preparing the tools",
    "text": "Preparing the tools\nwe‚Äôre going to use pandas, matplotlib and Numpy for data analysis and manipulation\n\n\nCode\n# import all the tools\n\n# Regular EDA(exploratory data analysis) and plotting library\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#we want our plots to appear inside the notebook\n%matplotlib inline \n\n# models from scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model evaluations \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import RocCurveDisplay"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#load-data",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#load-data",
    "title": "Heart disease prediction",
    "section": "Load data",
    "text": "Load data\n\n\nCode\ndf = pd.read_csv(\"data/heart-disease (1).csv\")\ndf.shape\n\n\n(303, 14)"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#data-exploration-exploratory-data-analysis-or-eda",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#data-exploration-exploratory-data-analysis-or-eda",
    "title": "Heart disease prediction",
    "section": "Data Exploration ( Exploratory data analysis or EDA)",
    "text": "Data Exploration ( Exploratory data analysis or EDA)\nThe goal here is to find out more about the data and become a subject matter export on the dataset you‚Äôre working with\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat‚Äôs missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\n\n\nCode\n#lets find out how many of each class exists\ndf.target.value_counts()\n\n\ntarget\n1    165\n0    138\nName: count, dtype: int64\n\n\n\n\nCode\ndf.target.value_counts().plot(kind = \"bar\", color = [\"salmon\",\"lightblue\"])\n\n\n&lt;Axes: xlabel='target'&gt;\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trestbps  303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalach   303 non-null    int64  \n 8   exang     303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slope     303 non-null    int64  \n 11  ca        303 non-null    int64  \n 12  thal      303 non-null    int64  \n 13  target    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n\n\n\n\nCode\ndf.isna().sum()\n\n\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ntarget      0\ndtype: int64\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\ncount\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n\n\nmean\n54.366337\n0.683168\n0.966997\n131.623762\n246.264026\n0.148515\n0.528053\n149.646865\n0.326733\n1.039604\n1.399340\n0.729373\n2.313531\n0.544554\n\n\nstd\n9.082101\n0.466011\n1.032052\n17.538143\n51.830751\n0.356198\n0.525860\n22.905161\n0.469794\n1.161075\n0.616226\n1.022606\n0.612277\n0.498835\n\n\nmin\n29.000000\n0.000000\n0.000000\n94.000000\n126.000000\n0.000000\n0.000000\n71.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n47.500000\n0.000000\n0.000000\n120.000000\n211.000000\n0.000000\n0.000000\n133.500000\n0.000000\n0.000000\n1.000000\n0.000000\n2.000000\n0.000000\n\n\n50%\n55.000000\n1.000000\n1.000000\n130.000000\n240.000000\n0.000000\n1.000000\n153.000000\n0.000000\n0.800000\n1.000000\n0.000000\n2.000000\n1.000000\n\n\n75%\n61.000000\n1.000000\n2.000000\n140.000000\n274.500000\n0.000000\n1.000000\n166.000000\n1.000000\n1.600000\n2.000000\n1.000000\n3.000000\n1.000000\n\n\nmax\n77.000000\n1.000000\n3.000000\n200.000000\n564.000000\n1.000000\n2.000000\n202.000000\n1.000000\n6.200000\n2.000000\n4.000000\n3.000000\n1.000000\n\n\n\n\n\n\n\n\n\nHeart disease frequency according to sex\n\n\nCode\ndf.sex.value_counts()\ndf.target.value_counts()\n\n\ntarget\n1    165\n0    138\nName: count, dtype: int64\n\n\n\n\nCode\n pd.crosstab(df.target, df.sex)\n\n\n\n\n\n\n\n\n\nsex\n0\n1\n\n\ntarget\n\n\n\n\n\n\n0\n24\n114\n\n\n1\n72\n93\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df.target,df.sex).plot(kind = \"bar\",\n                                  color =[\"salmon\",\"lightblue\"],\n                                  figsize =(10,6))\nplt.title(\"Heart Disease Frequency for sex\")\nplt.xlabel(\"0 = No disease, 1 = disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"]);\nplt.xticks(rotation=0)\n\n\n(array([0, 1]), [Text(0, 0, '0'), Text(1, 0, '1')])\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df.target, df.age)\n\n\n\n\n\n\n\n\n\nage\n29\n34\n35\n37\n38\n39\n40\n41\n42\n43\n...\n65\n66\n67\n68\n69\n70\n71\n74\n76\n77\n\n\ntarget\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n2\n0\n1\n1\n2\n1\n1\n3\n...\n4\n3\n6\n2\n1\n3\n0\n0\n0\n1\n\n\n1\n1\n2\n2\n2\n2\n3\n1\n9\n7\n5\n...\n4\n4\n3\n2\n2\n1\n3\n1\n1\n0\n\n\n\n\n2 rows √ó 41 columns\n\n\n\n\n\n\nCode\npd.crosstab(df.target,df.exang)\n\n\n\n\n\n\n\n\n\nexang\n0\n1\n\n\ntarget\n\n\n\n\n\n\n0\n62\n76\n\n\n1\n142\n23\n\n\n\n\n\n\n\n\n\n\nAge vs max heart rate for heart disease\n\n\nCode\n# create another figure\nplt.figure(figsize=(10,6))\n\n#scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1],\n           c=\"salmon\")\n\n#scatter with neg example\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           c=\"lightblue\");\n\nplt.title(\"Heart disease in function of age and max heart rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max heart rate\")\nplt.legend([\"Disease\",\"No disease\"]);\n\n\n\n\n\n\n\n\n\n\n\nCode\n#check the distribution of the age column with a histogram\ndf.age.plot.hist();\n\n\n\n\n\n\n\n\n\n\n\nHeart disease frequency per chest pain type\n\ncp - chest pain type\n\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related)\n\n\n\n\nCode\npd.crosstab(df.cp,df.target)\n\n\n\n\n\n\n\n\n\ntarget\n0\n1\n\n\ncp\n\n\n\n\n\n\n0\n104\n39\n\n\n1\n9\n41\n\n\n2\n18\n69\n\n\n3\n7\n16\n\n\n\n\n\n\n\n\n\n\nCode\n#make the crosstab more visual\npd.crosstab(df.cp,df.target).plot(kind=\"bar\",\n                                 figsize =(10,6),\n                                 color = [\"salmon\",\"lightblue\"])\nplt.title(\"Heart disease frequency per chest pain type\")\nplt.xlabel(\"Chest pain type\")\nplt.ylabel(\"amount\")\nplt.legend([\"No disease\",\"Disease\"])\nplt.xticks(rotation=0);\n\n\n\n\n\n\n\n\n\n\n\nCode\n# make a correlation matrix\ndf.corr()\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\nage\n1.000000\n-0.098447\n-0.068653\n0.279351\n0.213678\n0.121308\n-0.116211\n-0.398522\n0.096801\n0.210013\n-0.168814\n0.276326\n0.068001\n-0.225439\n\n\nsex\n-0.098447\n1.000000\n-0.049353\n-0.056769\n-0.197912\n0.045032\n-0.058196\n-0.044020\n0.141664\n0.096093\n-0.030711\n0.118261\n0.210041\n-0.280937\n\n\ncp\n-0.068653\n-0.049353\n1.000000\n0.047608\n-0.076904\n0.094444\n0.044421\n0.295762\n-0.394280\n-0.149230\n0.119717\n-0.181053\n-0.161736\n0.433798\n\n\ntrestbps\n0.279351\n-0.056769\n0.047608\n1.000000\n0.123174\n0.177531\n-0.114103\n-0.046698\n0.067616\n0.193216\n-0.121475\n0.101389\n0.062210\n-0.144931\n\n\nchol\n0.213678\n-0.197912\n-0.076904\n0.123174\n1.000000\n0.013294\n-0.151040\n-0.009940\n0.067023\n0.053952\n-0.004038\n0.070511\n0.098803\n-0.085239\n\n\nfbs\n0.121308\n0.045032\n0.094444\n0.177531\n0.013294\n1.000000\n-0.084189\n-0.008567\n0.025665\n0.005747\n-0.059894\n0.137979\n-0.032019\n-0.028046\n\n\nrestecg\n-0.116211\n-0.058196\n0.044421\n-0.114103\n-0.151040\n-0.084189\n1.000000\n0.044123\n-0.070733\n-0.058770\n0.093045\n-0.072042\n-0.011981\n0.137230\n\n\nthalach\n-0.398522\n-0.044020\n0.295762\n-0.046698\n-0.009940\n-0.008567\n0.044123\n1.000000\n-0.378812\n-0.344187\n0.386784\n-0.213177\n-0.096439\n0.421741\n\n\nexang\n0.096801\n0.141664\n-0.394280\n0.067616\n0.067023\n0.025665\n-0.070733\n-0.378812\n1.000000\n0.288223\n-0.257748\n0.115739\n0.206754\n-0.436757\n\n\noldpeak\n0.210013\n0.096093\n-0.149230\n0.193216\n0.053952\n0.005747\n-0.058770\n-0.344187\n0.288223\n1.000000\n-0.577537\n0.222682\n0.210244\n-0.430696\n\n\nslope\n-0.168814\n-0.030711\n0.119717\n-0.121475\n-0.004038\n-0.059894\n0.093045\n0.386784\n-0.257748\n-0.577537\n1.000000\n-0.080155\n-0.104764\n0.345877\n\n\nca\n0.276326\n0.118261\n-0.181053\n0.101389\n0.070511\n0.137979\n-0.072042\n-0.213177\n0.115739\n0.222682\n-0.080155\n1.000000\n0.151832\n-0.391724\n\n\nthal\n0.068001\n0.210041\n-0.161736\n0.062210\n0.098803\n-0.032019\n-0.011981\n-0.096439\n0.206754\n0.210244\n-0.104764\n0.151832\n1.000000\n-0.344029\n\n\ntarget\n-0.225439\n-0.280937\n0.433798\n-0.144931\n-0.085239\n-0.028046\n0.137230\n0.421741\n-0.436757\n-0.430696\n0.345877\n-0.391724\n-0.344029\n1.000000\n\n\n\n\n\n\n\n\n\n\nCode\n# lets make our correlation matrix visual\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                 annot = True,\n                 linewidths = 0.5,\n                 fmt = \".2f\",\n                 cmap = \"YlGnBu\");"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#modelling",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#modelling",
    "title": "Heart disease prediction",
    "section": "5. Modelling",
    "text": "5. Modelling\n\n\nCode\n# split into X & y\nX = df.drop(\"target\", axis =1)\ny = df[\"target\"]\n\n\n\n\nCode\nX\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n298\n57\n0\n0\n140\n241\n0\n1\n123\n1\n0.2\n1\n0\n3\n\n\n299\n45\n1\n3\n110\n264\n0\n1\n132\n0\n1.2\n1\n0\n3\n\n\n300\n68\n1\n0\n144\n193\n1\n1\n141\n0\n3.4\n1\n2\n3\n\n\n301\n57\n1\n0\n130\n131\n0\n1\n115\n1\n1.2\n1\n1\n3\n\n\n302\n57\n0\n1\n130\n236\n0\n0\n174\n0\n0.0\n1\n1\n2\n\n\n\n\n303 rows √ó 13 columns\n\n\n\n\n\n\nCode\ny\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n298    0\n299    0\n300    0\n301    0\n302    0\nName: target, Length: 303, dtype: int64\n\n\n\n\nCode\n# split data into train and test set\nnp.random.seed(42)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2)\n\n\n\n\nCode\nX_train\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n132\n42\n1\n1\n120\n295\n0\n1\n162\n0\n0.0\n2\n0\n2\n\n\n202\n58\n1\n0\n150\n270\n0\n0\n111\n1\n0.8\n2\n0\n3\n\n\n196\n46\n1\n2\n150\n231\n0\n1\n147\n0\n3.6\n1\n0\n2\n\n\n75\n55\n0\n1\n135\n250\n0\n0\n161\n0\n1.4\n1\n0\n2\n\n\n176\n60\n1\n0\n117\n230\n1\n1\n160\n1\n1.4\n2\n2\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n188\n50\n1\n2\n140\n233\n0\n1\n163\n0\n0.6\n1\n1\n3\n\n\n71\n51\n1\n2\n94\n227\n0\n1\n154\n1\n0.0\n2\n1\n3\n\n\n106\n69\n1\n3\n160\n234\n1\n0\n131\n0\n0.1\n1\n1\n2\n\n\n270\n46\n1\n0\n120\n249\n0\n0\n144\n0\n0.8\n2\n0\n3\n\n\n102\n63\n0\n1\n140\n195\n0\n1\n179\n0\n0.0\n2\n2\n2\n\n\n\n\n242 rows √ó 13 columns\n\n\n\n\nNow we‚Äôve got our data split into train and test sets, its time to use machine learning\nWe‚Äôre going to try 3 different machine learning model 1. Logistic regression 2. K-Nearest Neighbours classifier 3. Random forest classifier\n\n\nCode\n# put models in a dictionary\nmodels = {\"logistics regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# creare a function to fit and score models \ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different scikit-learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test label\n    \"\"\"\n    #set random seed\n    np.random.seed(42)\n    \n    #set empty dict to store model scores\n    model_scores = {}\n    \n    # looping through model dict\n    for name, model in models.items():\n        # fit the model\n        model.fit(X_train, y_train)\n        #evaluate the model\n        model_scores [name] = model.score(X_test, y_test)\n    return model_scores\n\n\n\n\nCode\nmodel_scores = fit_and_score(models, X_train,X_test,y_train,y_test)\nmodel_scores\n\n\nc:\\Users\\sauga\\OneDrive\\Desktop\\AI & machine learning\\blog\\quarto-env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n{'logistics regression': 0.8852459016393442,\n 'KNN': 0.6885245901639344,\n 'Random Forest': 0.8360655737704918}\n\n\n\nModel comparison\n\n\nCode\nmodel_compare = pd.DataFrame(model_scores, index = [\"accuracy\"])\nmodel_compare.T.plot.bar();\n\n\n\n\n\n\n\n\n\nNow we‚Äôve got a baseline model and we know model‚Äôs first predicction aren‚Äôt always final\nlets look at the following: * Hyperparameter tuning * Feature importance * Confusion matrix * Cross-validation * Precision * Recall * F1 score * Classification report * ROC vurve * Area under the curve (AUC)\n\n\nHyperparameter tuning\n\n\nCode\n#let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# create a list of different values for n-neighbours \nneighbors = range(1,21)\n\n# setup KNN instance\nknn = KNeighborsClassifier()\n\n# loop for different value of n-neighbours\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    #fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    #update the training score \n    train_scores.append(knn.score(X_train,y_train))\n    \n    #update the test score \n    test_scores.append(knn.score(X_test, y_test))\n\n\n\n\nCode\ntrain_scores\n\n\n[1.0,\n 0.8099173553719008,\n 0.7727272727272727,\n 0.743801652892562,\n 0.7603305785123967,\n 0.7520661157024794,\n 0.743801652892562,\n 0.7231404958677686,\n 0.71900826446281,\n 0.6942148760330579,\n 0.7272727272727273,\n 0.6983471074380165,\n 0.6900826446280992,\n 0.6942148760330579,\n 0.6859504132231405,\n 0.6735537190082644,\n 0.6859504132231405,\n 0.6652892561983471,\n 0.6818181818181818,\n 0.6694214876033058]\n\n\n\n\nCode\nplt.plot(neighbors, train_scores, label = \"train score\")\nplt.plot(neighbors, test_scores, label = \"test score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"number of neighbors\")\nplt.ylabel(\"model score\")\nplt.legend()\n\nprint(f\"The best accuracy of the KNN model is {max(test_scores)*100:.2f} %\")\n\n\nThe best accuracy of the KNN model is 75.41 %"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-tuning-with-randomizedsearchcv",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-tuning-with-randomizedsearchcv",
    "title": "Heart disease prediction",
    "section": "hyperparameter tuning with randomizedsearchCV",
    "text": "hyperparameter tuning with randomizedsearchCV\nwe‚Äôre going to tune: * LogisticRegression() * RandomForestClassifier()\nusing randomizedsearchCV\n\n\nCode\n# create hyperparamter grid for logisticRegression\nlog_reg_grid = {\"C\" : np.logspace(-4,4,20),\n               \"solver\" : [\"liblinear\"]}\n\n#create hyperparameter grid for RandomSearchCV\nrf_grid = {\"n_estimators\" : np.arange(10,1000,50),\n          \"max_depth\" : [None, 3,5,10],\n          \"min_samples_split\" : np.arange(2,20,2),\n          \"min_samples_leaf\" : np.arange(1,20,2)}\n\n\nlets use RandomizedSearchCV\n\n\nCode\n# tune logisticRegression \n  \nnp.random.seed(42)\n\n# set up random hyperparameter for logisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv =5,\n                               n_iter = 20,\n                               verbose = True)\n\n#fit the model\nrs_log_reg.fit(X_train, y_train)\n\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\nRandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=20,\n                   param_distributions={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                                        'solver': ['liblinear']},\n                   verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=20,\n                   param_distributions={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                                        'solver': ['liblinear']},\n                   verbose=True)estimator: LogisticRegressionLogisticRegression()LogisticRegressionLogisticRegression()\n\n\n\n\nCode\nrs_log_reg.best_params_\n\n\n{'solver': 'liblinear', 'C': 0.23357214690901212}\n\n\n\n\nCode\nrs_log_reg.score(X_test, y_test)\n\n\n0.8852459016393442\n\n\n\n\nCode\n# tune RandomForestClassifier\n  \nnp.random.seed(42)\n\n# set up random hyperparameter for randomforestclassifier\nrs_rf= RandomizedSearchCV(RandomForestClassifier(),\n                               param_distributions=rf_grid,\n                               cv =5,\n                               n_iter = 20,\n                               verbose = True)\n\n#fit the model\nrs_rf.fit(X_train, y_train)\n\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=20,\n                   param_distributions={'max_depth': [None, 3, 5, 10],\n                                        'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n                                        'n_estimators': array([ 10,  60, 110, 160, 210, 260, 310, 360, 410, 460, 510, 560, 610,\n       660, 710, 760, 810, 860, 910, 960])},\n                   verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=20,\n                   param_distributions={'max_depth': [None, 3, 5, 10],\n                                        'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n                                        'n_estimators': array([ 10,  60, 110, 160, 210, 260, 310, 360, 410, 460, 510, 560, 610,\n       660, 710, 760, 810, 860, 910, 960])},\n                   verbose=True)estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\n\n\n\nCode\nrs_rf.best_params_\n\n\n{'n_estimators': 210,\n 'min_samples_split': 4,\n 'min_samples_leaf': 19,\n 'max_depth': 3}\n\n\n\n\nCode\nrs_rf.score(X_test,y_test)\n\n\n0.8688524590163934"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-turning-using-gridsearchcv",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-turning-using-gridsearchcv",
    "title": "Heart disease prediction",
    "section": "Hyperparameter turning using GridSearchCV",
    "text": "Hyperparameter turning using GridSearchCV\n\n\nCode\n# different hyperparameter for our logisticRegression Model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n                \"solver\":[\"liblinear\"]\n               }\n\nnp.random.seed(42)\n\n#setup grid hyperparameter search for logisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv =5,\n                         verbose = True)\n\n#fit our model\ngs_log_reg.fit(X_train,y_train);\n\n\nFitting 5 folds for each of 30 candidates, totalling 150 fits\n\n\n\n\nCode\n# check best hyperparameter\ngs_log_reg.best_params_\n\n\n{'C': 0.20433597178569418, 'solver': 'liblinear'}\n\n\n\n\nCode\ngs_log_reg.score(X_test,y_test)\n\n\n0.8852459016393442"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluating-our-tuned-machine-learning-classfier-beyond-accuracy",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluating-our-tuned-machine-learning-classfier-beyond-accuracy",
    "title": "Heart disease prediction",
    "section": "Evaluating our tuned machine learning classfier, beyond accuracy",
    "text": "Evaluating our tuned machine learning classfier, beyond accuracy\n\nROC curve and AUC score\nConfusion matrix\nClassification report\nPrecision\nRecall\nF1-score\n\nTo make comparisons and evaluate our trained model, first we need to make predictions\n\n\nCode\n# make prediction with tuned model\ny_preds = gs_log_reg.predict(X_test)\n\n\n\n\nCode\ny_preds\n\n\narray([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)\n\n\n\n\nCode\n# plot ROC curve and calculate AUC score\nRocCurveDisplay.from_estimator(gs_log_reg,X_test,y_test);\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Confusion matrix \nprint(confusion_matrix(y_test,y_preds))\n\n\n[[25  4]\n [ 3 29]]\n\n\n\n\nCode\nsns.set(font_scale= 1.5)\n\ndef plot_conf_mat(y_test,y_preds):\n    \"\"\"\n    PLots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot = True,\n                    cbar = False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test,y_preds)\n\n\n\n\n\n\n\n\n\nlets get classification report as well as cross-validated precision, recall and f1 score.\n\n\nCode\nprint(classification_report(y_test,y_preds))\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.86      0.88        29\n           1       0.88      0.91      0.89        32\n\n    accuracy                           0.89        61\n   macro avg       0.89      0.88      0.88        61\nweighted avg       0.89      0.89      0.89        61\n\n\n\n\nCalculate evaluation metrics using cross validation\n\n\nCode\n# check best hyperparameters \ngs_log_reg.best_params_\n\n\n{'C': 0.20433597178569418, 'solver': 'liblinear'}\n\n\n\n\nCode\n# create a new classifier with best params \nclf = LogisticRegression(C=0.20433597178569418,\n                        solver = \"liblinear\")\n\n\n\n\nCode\n# cross validated accuracy\ncv_acc = cross_val_score(clf, X, y, cv = 5, scoring= \"accuracy\")\ncv_acc\n\n\narray([0.81967213, 0.90163934, 0.8852459 , 0.88333333, 0.75      ])\n\n\n\n\nCode\ncv_acc = np.mean(cv_acc)\ncv_acc\n\n\n0.8479781420765027\n\n\n\n\nCode\n# cross validated precision\ncv_precision =cross_val_score(clf, X, y, cv = 5, scoring= \"precision\")\n\ncv_precision = np.mean(cv_precision)\ncv_precision\n\n\n0.8215873015873015\n\n\n\n\nCode\n# cross validated recall \ncv_recall = cross_val_score(clf, X, y, cv = 5, scoring= \"recall\")\n\ncv_recall = np.mean(cv_recall)\ncv_recall\n\n\n0.9272727272727274\n\n\n\n\nCode\n# cross validated f1-score\ncv_f1 = cross_val_score(clf, X, y, cv = 5, scoring= \"precision\")\n\ncv_f1 = np.mean(cv_f1)\ncv_f1\n\n\n0.8215873015873015\n\n\n\n\nCode\n# visualize cross validated metrics \ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                          \"Precision\" : cv_precision,\n                          \"recall\" : cv_recall,\n                          \"f1-score\" : cv_f1},\n                          index = [0])\ncv_metrics.T.plot.bar(title = \"Cross validated evaluation metrics\",legend = 0);\n\n\n\n\n\n\n\n\n\n\n\nfeature importance\nWhich feature contributed the most to the outcomes of the model and how did they contribute\n\n\nCode\n# fit an instance of logisticregressionabs\nclf.fit(X_train,y_train);\n\n\n\n\nCode\n# check coef_\nclf.coef_\n\n\narray([[ 0.00320769, -0.86062047,  0.66001431, -0.01155971, -0.00166496,\n         0.04017239,  0.31603402,  0.02458922, -0.6047017 , -0.56795457,\n         0.45085391, -0.63733326, -0.6755509 ]])\n\n\n\n\nCode\n# match coef's of features to columns \nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict\n\n\n{'age': 0.0032076873709286024,\n 'sex': -0.8606204735539111,\n 'cp': 0.6600143086174385,\n 'trestbps': -0.01155970641957489,\n 'chol': -0.0016649609500147373,\n 'fbs': 0.04017238940156104,\n 'restecg': 0.3160340177157746,\n 'thalach': 0.02458922261936637,\n 'exang': -0.6047017032281077,\n 'oldpeak': -0.567954572983317,\n 'slope': 0.4508539117301764,\n 'ca': -0.6373332602422034,\n 'thal': -0.6755508982355707}\n\n\n\n\nCode\n# visualize feature importance\nfeature_df = pd.DataFrame(feature_dict,index=[0])\nfeature_df.T.plot.bar(title = \"Feature Importance\", legend = 0\n                     );"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#experimentation",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#experimentation",
    "title": "Heart disease prediction",
    "section": "6. Experimentation",
    "text": "6. Experimentation\nIf the evaluation metrics is not met:\n\nCould you collect more data?\nCould you try better model? like catboost or XGBoost?\nCould we improve the current model ? (beyond what we have done so far)\nif you meet your evaluation metrics, how would you share it with others?"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nLarge Language Model have generated a shift in our daily life. We can already see the impact of LLMs based appilcation like ChatGPT in our daily lives.\nThe fundamental working principle behind such LLMs has been deep learning. I am grateful enough to live in this period where we can leverage such powerful models to do anything that involves natural language.\nAnd this is exciting as I begin to learn more about language models, machine learning, AI and whatever the heck that comes out in the future. I feel like this is the right time to get into machine learning and AI for anyone literally.\n source: 5 things you should know about AI.(Cambridge consultant, 2017)\nHere I will share my journey of becoming an AI developer, bringing AI and Machine learning based product to production in real world."
  },
  {
    "objectID": "posts/ml-specialization/index.html",
    "href": "posts/ml-specialization/index.html",
    "title": "Supervised Machine learning : Regression and classification",
    "section": "",
    "text": "Overview of machine learning\n\n\nApplications of machine learning\n\nEverywhere\n\n\n\n\n\nSupervised vs Unsupervised machine learning\n\n\n\n\nSupervised learning\n\n\nLearns from being given the ‚Äúright answer‚Äù  Application of supervised learning  What‚Äôs supervised learning?\n\n\n\n\nUnsupervised learning\n\n\n Unsupervised learning example\n\nClustering: Group similar data points together\nAnomaly detection: Find unusual datapoint\nDimensionality reduction: Compress data using fewer data\n\n\n\n\n\n\n\nLinear regression model\n\n\nWhat is linear regression model?\n\nFit a line or model to the training data such that it gives minimum error (cost function). Model parameters are adjusted to get the desired model.  Goal of linear regression\n\nExample: House sizes and prices\nWhat is cost function?\n\nIt helps us visualize how well our model is predicting by comparing the value predicted by the model to the actual target data in the training set. The cost function may vary according to the model.  Linear regression model\nFor example: Generally for linear regression model, mean squared error is used as cost function  Squared Error Cost function  Cost function visualization\n\n\n\n\n\nTrain the model with gradient descent\n\n\nWhat is gradient descent?\n\nGoing down hill to local minima  Gradient descent intuition\n\nWhat‚Äôs the initial guess?\n\nIn linear regression mostly w and b are set to zero initially and reduce the cost function over multiple iterations.  Gradient descent implementation  The derivative term in gradient descent formula\n\nWhat is learning rate?  Learning rate\nWhat if the parameter already reaches local minima? No change  Local minima  Example of gradient descent for linear regression  Batch gradient descent\nLab: Implement gradient descent algorithm"
  },
  {
    "objectID": "posts/ml-specialization/index.html#week-1",
    "href": "posts/ml-specialization/index.html#week-1",
    "title": "Supervised Machine learning : Regression and classification",
    "section": "",
    "text": "Overview of machine learning\n\n\nApplications of machine learning\n\nEverywhere\n\n\n\n\n\nSupervised vs Unsupervised machine learning\n\n\n\n\nSupervised learning\n\n\nLearns from being given the ‚Äúright answer‚Äù  Application of supervised learning  What‚Äôs supervised learning?\n\n\n\n\nUnsupervised learning\n\n\n Unsupervised learning example\n\nClustering: Group similar data points together\nAnomaly detection: Find unusual datapoint\nDimensionality reduction: Compress data using fewer data\n\n\n\n\n\n\n\nLinear regression model\n\n\nWhat is linear regression model?\n\nFit a line or model to the training data such that it gives minimum error (cost function). Model parameters are adjusted to get the desired model.  Goal of linear regression\n\nExample: House sizes and prices\nWhat is cost function?\n\nIt helps us visualize how well our model is predicting by comparing the value predicted by the model to the actual target data in the training set. The cost function may vary according to the model.  Linear regression model\nFor example: Generally for linear regression model, mean squared error is used as cost function  Squared Error Cost function  Cost function visualization\n\n\n\n\n\nTrain the model with gradient descent\n\n\nWhat is gradient descent?\n\nGoing down hill to local minima  Gradient descent intuition\n\nWhat‚Äôs the initial guess?\n\nIn linear regression mostly w and b are set to zero initially and reduce the cost function over multiple iterations.  Gradient descent implementation  The derivative term in gradient descent formula\n\nWhat is learning rate?  Learning rate\nWhat if the parameter already reaches local minima? No change  Local minima  Example of gradient descent for linear regression  Batch gradient descent\nLab: Implement gradient descent algorithm"
  },
  {
    "objectID": "posts/ml-specialization/index.html#week-2-june-1-2023",
    "href": "posts/ml-specialization/index.html#week-2-june-1-2023",
    "title": "Supervised Machine learning : Regression and classification",
    "section": "Week 2 June 1, 2023",
    "text": "Week 2 June 1, 2023\n\n\nMultiple linear regression\n\n - vectorization - why is vectorization fast? - makes code shorter & faster due to parallel computing capabilities using numpy and gpus  - Behind the scenes  - Alternative to gradient descent  - Gradient descent for multiple regression \n\n\n\nGradient descent in practice\n\n\nfeature scaling\n\nlarge features ‚Üí small parameter values\nsmall features ‚Üí large parameter values\n\n\n\nUntitled\n\n\nscaling the feature to lie in similar range results in faster gradient descent\n\n\n\nUntitled\n\n\n\n\n\nUntitled\n\n\nmethods for scaling:\n\nnormalization : divide by max value of parameter\nmean normalization\nz-normalization\n\n\nChecking gradient descent for convergence\n\n\n\n\nUntitled\n\n\n\n\n\nUntitled\n\n\n\nFeature engineering\n\n\n\n\nUntitled\n\n\n\nPolynomial regression\n\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/ml-specialization/index.html#week-3",
    "href": "posts/ml-specialization/index.html#week-3",
    "title": "Supervised Machine learning : Regression and classification",
    "section": "Week 3",
    "text": "Week 3\n\n\nClassification with Logistic Regression\n\n\nLogistic regression is applied for classification problems.\n\nWhy not linear regression for classification problems? \nLogistic regression from sigmoid function \\(f_w, b (x)\\) is a sigmoid function instead of a linear function. \nThe output of logistic regression is expressed in terms of probability \nDecision boundary Visualize logistic regression - pick a threshold value. \nDecision boundary - straight line. \n\n\n\nNon-linear decision boundaries\n\n\n\n\n\nCost Function for Logistic Regression\n\nWhy does the mean squared error cost don‚Äôt work for logistic regression? \nLogistic loss function Loss function measures model output for one training example whereas the cost function sums up loss function for each training example.\nFor true label: y = 1 \nFor false label: y = 0 \n\n\n\nLogistic cost function\n\n\nSimplified cost function. \n\n\n\nGradient Descent for Logistic Regression\n\n\n\n\nGradient descent for logistic regression\n\n\nSimilarity between GD for linear regression & logistic regression. \n\n\n\nThe Problem of Overfitting\n\nUnderfitting vs Generalization vs Overfitting in Regression \nUnderfitting vs Generalization vs Overfitting in Classification \nHow to address overfitting problem? 1. By collecting more data. 2. By relevant feature selection.  3. By regularization. \nCost function with regularization Intuition for regularization. \nConventionally regularization is done for the w parameter only. \nChoosing the right value for lambda - If lambda is very large ‚áí underfit. - If lambda is very small ‚áí overfit. \nRegularized linear regression \nRegularized logistic regression Similar to linear regression."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "quarto-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "quarto-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "UAT 13 - Animation",
    "text": "UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\nUAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\nUAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\nUAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\nUAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both ‚Äúregular‚Äù and ‚Äúsingle-shot‚Äù timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\nUAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it‚Äôs comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\nUAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Autokill.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Autokill.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "import os\nimport signal\npid = os.getpid()\nos.kill(pid, signal.SIGTERM)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Check History in Memory.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Check History in Memory.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "from IPython import get_ipython\n\n\nip = get_ipython()\nassert ip.history_manager.hist_file == ':memory:'"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Clear Output.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Clear Output.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "from __future__ import print_function\nfrom IPython.display import clear_output\n\n\nfor i in range(10):\n    clear_output()\n    print(i)\n\n9\n\n\n\nprint(\"Hello world\")\nclear_output()\n\n\nprint(\"Hello world\", end='')\nclear_output(wait=True)  # no output after this\n\nHello world\n\n\n\nprint(\"Hello\", end='')\nclear_output(wait=True)  # here we have new output after wait=True\nprint(\"world\", end='')\n\nworld\n\n\n\nhandle0 = display(\"Hello world\", display_id=\"id0\")\n\n'Hello world'\n\n\n\nhandle1 = display(\"Hello\", display_id=\"id1\")\n\n'world'\n\n\n\nhandle1.update('world')\n\n\nhandle2 = display(\"Hello world\", display_id=\"id2\")\nclear_output()  # clears all output, also with display_ids\n\n\nhandle3 = display(\"Hello world\", display_id=\"id3\")\nclear_output(wait=True)\n\n'Hello world'\n\n\n\nhandle4 = display(\"Hello\", display_id=\"id4\")\nclear_output(wait=True)\nprint('world', end='')\n\nworld\n\n\n\nhandle4.update('Hello world')  # it is cleared, so it should not show up in the above cell"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Disable Stdin.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Disable Stdin.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "try:\n    input = raw_input\nexcept:\n    pass\n\nname = input(\"name: \")"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Empty Cell.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Empty Cell.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "Test that executing skips over an empty cell.\n\n\"Code 1\"\n\n'Code 1'\n\n\n\n\"Code 2\"\n\n'Code 2'"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Error.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Error.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "0/0\n\nZeroDivisionError: division by zero"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Factorials.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Factorials.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "i, j = 1, 1\n\n\nfor m in range(10):\n    i, j = j, i + j\n    print(j)\n\n2\n3\n5\n8\n13\n21\n34\n55\n89\n144"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/HelloWorld.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/HelloWorld.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "print(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Inline Image.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Inline Image.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "from IPython.display import Image\n\n\nImage('python.png')"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Interrupt.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Interrupt.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "while True: continue\n\nKeyboardInterrupt: \n\n\n\nprint(\"done\")\n\ndone"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/JupyterWidgets.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/JupyterWidgets.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "import ipywidgets\nlabel = ipywidgets.Label('Hello World')\ndisplay(label)\n\n\n\n\n\n# it should also handle custom msg'es\nlabel.send({'msg': 'Hello'})"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Other Comms.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Other Comms.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "from comm import create_comm\n\n\ncomm = create_comm('this-comm-tests-a-missing-handler', data={'id': 'foo'})\n\n\ncomm.send(data={'id': 'bar'})"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Output.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Output.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "import ipywidgets as widgets\nfrom IPython.display import clear_output\noutput1 = widgets.Output()\noutput1\n\n\n\n\n\nprint(\"hi\")\nwith output1:\n    print(\"in output\")\n\nhi\n\n\n\nwith output1:\n    raise ValueError(\"trigger msg_type=error\")\n\n\nimport ipywidgets as widgets\noutput2 = widgets.Output()\noutput2\n\n\n\n\n\nprint(\"hi2\")\nwith output2:\n    print(\"in output2\")\n    clear_output(wait=True)\n\nhi2\n\n\n\nimport ipywidgets as widgets\noutput3 = widgets.Output()\noutput3\n\n\n\n\n\nprint(\"hi3\")\nwith output3:\n    print(\"hello\")\n    clear_output(wait=True)\n    print(\"world\")\n\nhi3\n\n\n\nimport ipywidgets as widgets\noutput4 = widgets.Output()\noutput4\n\n\n\n\n\nprint(\"hi4\")\nwith output4:\n    print(\"hello world\")\n    clear_output()\n\nhi4\n\n\n\nimport ipywidgets as widgets\noutput5 = widgets.Output()\noutput5\n\n\n\n\n\nprint(\"hi5\")\nwith output5:\n    display(\"hello world\") # this is not a stream but plain text\nclear_output()\n\n\nimport ipywidgets as widgets\noutput_outer = widgets.Output()\noutput_inner = widgets.Output()\noutput_inner\n\n\n\n\n\noutput_outer\n\n\n\n\n\nwith output_inner:\n    print('in inner')\n    with output_outer:\n        print('in outer')\n    print('also in inner')"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Parallel Execute A.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Parallel Execute A.html",
    "title": "Ensure notebooks can execute in parallel",
    "section": "",
    "text": "This notebook uses a file system based ‚Äúlock‚Äù to assert that two instances of the notebook kernel will run in parallel. Each instance writes to a file in a temporary directory, and then tries to read the other file from the temporary directory, so that running them in sequence will fail, but running them in parallel will succeed.\nTwo notebooks are launched, each which sets the this_notebook variable. One notebook is set to this_notebook = 'A' and the other this_notebook = 'B'.\n\nimport os\nimport os.path\nimport tempfile\nimport time\n\n\n# the variable this_notebook is injectected in a cell above by the test framework.\nthis_notebook = 'A'\nother_notebook = 'B'\ndirectory = os.environ['NBEXECUTE_TEST_PARALLEL_TMPDIR']\nwith open(os.path.join(directory, 'test_file_{}.txt'.format(this_notebook)), 'w') as f:\n    f.write('Hello from {}'.format(this_notebook))\n\n\nstart = time.time()\ntimeout = 5\nend = start + timeout\ntarget_file = os.path.join(directory, 'test_file_{}.txt'.format(other_notebook))\nwhile time.time() &lt; end:\n    time.sleep(0.1)\n    if os.path.exists(target_file):\n        with open(target_file, 'r') as f:\n            text = f.read()\n        if text == 'Hello from {}'.format(other_notebook):\n            break\nelse:\n    assert False, \"Timed out ‚Äì¬†didn't get a message from {}\".format(other_notebook)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Parallel Execute B.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Parallel Execute B.html",
    "title": "Ensure notebooks can execute in parallel",
    "section": "",
    "text": "This notebook uses a file system based ‚Äúlock‚Äù to assert that two instances of the notebook kernel will run in parallel. Each instance writes to a file in a temporary directory, and then tries to read the other file from the temporary directory, so that running them in sequence will fail, but running them in parallel will succeed.\nTwo notebooks are launched, each which sets the this_notebook variable. One notebook is set to this_notebook = 'A' and the other this_notebook = 'B'.\n\nimport os\nimport os.path\nimport tempfile\nimport time\n\n\n# the variable this_notebook is injectected in a cell above by the test framework.\nthis_notebook = 'B'\nother_notebook = 'A'\ndirectory = os.environ['NBEXECUTE_TEST_PARALLEL_TMPDIR']\nwith open(os.path.join(directory, 'test_file_{}.txt'.format(this_notebook)), 'w') as f:\n    f.write('Hello from {}'.format(this_notebook))\n\n\nstart = time.time()\ntimeout = 5\nend = start + timeout\ntarget_file = os.path.join(directory, 'test_file_{}.txt'.format(other_notebook))\nwhile time.time() &lt; end:\n    time.sleep(0.1)\n    if os.path.exists(target_file):\n        with open(target_file, 'r') as f:\n            text = f.read()\n        if text == 'Hello from {}'.format(other_notebook):\n            break\nelse:\n    assert False, \"Timed out ‚Äì¬†didn't get a message from {}\".format(other_notebook)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Exceptions with Cell Tags.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Exceptions with Cell Tags.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "import sys\nprint(\"hello\")\nprint(\"errorred\", file=sys.stderr)\n# √º√±√Æ√ß√∏‚àÇ√©\nraise Exception(\"message\")\n\nhello\n\n\nerrorred\n\n\nException: message\n\n\n\nprint('ok')\n\nok"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Exceptions.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Exceptions.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "# √º√±√Æ√ß√∏‚àÇ√©\nraise Exception(\"message\")\n\nException: message\n\n\n\nprint('ok')\n\nok"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Execution with Cell Tag.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Skip Execution with Cell Tag.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "print(\"a long running cell\")\n\n\nprint('ok')\n\nok"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Sleep1s.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Sleep1s.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "import time\nimport datetime\n\n\nt0 = datetime.datetime.utcnow()\ntime.sleep(1)\nt1 = datetime.datetime.utcnow()\n\n\ntime_format = '%Y-%m-%dT%H:%M:%S.%fZ'\nprint(t0.strftime(time_format), end='')\n\n\nprint(t1.strftime(time_format), end='')"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/SVG.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/SVG.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "from IPython.display import SVG\n\n\nSVG(data='''\n&lt;svg height=\"100\" width=\"100\"&gt;\n    &lt;circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" stroke-width=\"2\" fill=\"red\" /&gt;\n&lt;/svg&gt;''')"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/Unicode.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/Unicode.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "print('‚òÉ')\n\n‚òÉ"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/UnicodePy3.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/UnicodePy3.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "print('‚òÉ')\n\n‚òÉ"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/nbclient/tests/files/update-display-id.html",
    "href": "quarto-env/Lib/site-packages/nbclient/tests/files/update-display-id.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "ip = get_ipython()\n\nfrom IPython.display import display\n\ndef display_with_id(obj, display_id, update=False, execute_result=False):\n    iopub = ip.kernel.iopub_socket\n    session = get_ipython().kernel.session\n    data, md = ip.display_formatter.format(obj)\n    transient = {'display_id': str(display_id)}\n    content = {'data': data, 'metadata': md, 'transient': transient}\n    if execute_result:\n      msg_type = 'execute_result'\n      content['execution_count'] = ip.execution_count\n    else:\n      msg_type = 'update_display_data' if update else 'display_data'\n    session.send(iopub, msg_type, content, parent=ip.parent_header)\n\n\ndisplay('above')\ndisplay_with_id(1, 'here')\ndisplay('below')\n\n'above'\n\n\n8\n\n\n'below'\n\n\n\ndisplay_with_id(2, 'here')\ndisplay_with_id(3, 'there')\ndisplay_with_id(4, 'here')\n\n8\n\n\n6\n\n\n8\n\n\n\ndisplay_with_id(5, 'there')\ndisplay_with_id(6, 'there', update=True)\n\n6\n\n\n\ndisplay_with_id(7, 'here')\ndisplay_with_id(8, 'here', update=True)\ndisplay_with_id(9, 'result', execute_result=True)\n\n8\n\n\n10\n\n\n\ndisplay_with_id(10, 'result', update=True)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm‚Äôs designer. Component licenses are located with the component code."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/pyzmq-25.1.1.dist-info/AUTHORS.html",
    "href": "quarto-env/Lib/site-packages/pyzmq-25.1.1.dist-info/AUTHORS.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren G√ºven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nF√©lix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonu»õ Ar»õƒÉri»ôi (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Pi√´l (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jƒôdrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/pyzmq-25.1.1.dist-info/AUTHORS.html#authors",
    "href": "quarto-env/Lib/site-packages/pyzmq-25.1.1.dist-info/AUTHORS.html#authors",
    "title": "Saugat's blog",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren G√ºven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nF√©lix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonu»õ Ar»õƒÉri»ôi (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Pi√´l (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jƒôdrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/seaborn-0.13.0.dist-info/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/seaborn-0.13.0.dist-info/LICENSE.html",
    "title": "Saugat's blog",
    "section": "",
    "text": "Copyright (c) 2012-2021, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/ml-specialization/index.html#week-2",
    "href": "posts/ml-specialization/index.html#week-2",
    "title": "Supervised Machine learning : Regression and classification",
    "section": "Week 2",
    "text": "Week 2\n\n\nMultiple linear regression\n\n\n\n\nMultiple linear regression\n\n\nVectorization - Why is vectorization fast? - Makes code shorter & faster due to parallel computing capabilities using numpy and GPUs \nBehind the scenes \n\nAlternative to gradient descent \nGradient descent for multiple regression \n\n\n\n\nGradient descent in practice\n\n\nFeature scaling\n\nLarge features ‚Üí small parameter values\nSmall features ‚Üí large parameter values\n\n\n\n\nFeature scaling\n\n\n\n\n\nImpact of feature size on parameter size\n\n\nScaling the feature to lie in a similar range results in a faster gradient descent \n Methods for scaling:\n\nNormalization: divide by the max value of the parameter\nMean normalization \nZ-normalization \n\nChecking gradient descent for convergence  How to choose the right learning rate? \nFeature engineering \nPolynomial regression"
  },
  {
    "objectID": "posts/ml-specialization2/index.html",
    "href": "posts/ml-specialization2/index.html",
    "title": "Advanced learning algorithm",
    "section": "",
    "text": "Neural network intuition\n\n\nbrief origin of neural networks\n\n\n\n\norigin of neural network\n\n\n\norigin of neural network\n\n\n\n\nwhy now?\n\n\n\nExample:\nsingle hidden layer\n\nThere is input layer with vector x which carries input features. these input are given to a hidden layer which outputs three activation values in the form of vector a, which is an input to output layer.\nThe neural network is smart enough to identify the features: affordability, awareness and perceived quality based on the input values.\n\n\n\nUntitled\n\n\n\n\n\ndemand prediction using neural network\n\n\ndemand prediction using neural network\nmultiple hidden layers - multilayer perceptron\n\n\n\nmultilayer perceptron\n\n\nFace recognition\n\n\n\nExample\n\n\n\n- Neural network model\n    \n    **How does a neural network layer work?**\n    \n    ![looking inside layer 1](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%206.png)\n    \n    looking inside layer 1\n    \n    ![inside layer 2](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%207.png)\n    \n    inside layer 2\n    \n    **Activation value for a given layer**\n    \n    ![notation for activation value/function](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%208.png)\n    \n    notation for activation value/function\n    \n    **forward propagation example:**\n    \n    computation goes from left to right. more hidden layers initially\n    \n    ![8*8 pixel image of handwritten digit](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%209.png)\n    \n    8*8 pixel image of handwritten digit\n    \n    compute a_1 and a_2\n    \n    ![compute activation value](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2010.png)\n    \n    compute activation value\n    \n    ![handwritten digit recognition with forward propagation](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2011.png)\n    \n    handwritten digit recognition with forward propagation\n    \n- TensorFlow implementation\n    \n    ![coffee roasting model](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2012.png)\n    \n    coffee roasting model\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2013.png)\n    \n    **tensorflow implementation of digit classification**\n    \n    ![digit classification inference](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2014.png)\n    \n    digit classification inference\n    \n    **data in tensorflow**\n    \n    ![2D numpy array ‚Üí matrix](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2015.png)\n    \n    2D numpy array ‚Üí matrix\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2016.png)\n    \n    ![tensorflow representation of data](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2017.png)\n    \n    tensorflow representation of data\n    \n    **Building a neural network architecture**\n    \n    ![neural network architecture using tensorflow](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2018.png)\n    \n    neural network architecture using tensorflow\n    \n    - **Here by convention the model is written as**\n        \n        ```python\n        model = Sequential([\n                Dense(units = 3, activation = \"sigmoid\"),\n                Dense(units = 1, activation = \"sigmoid\")])\n        ```\n        \n    - **Implementing NN architecture for digit classification using tensorflow**\n        \n        ```python\n        model = Sequential([\n                Dense(units = 25, activation = \"sigmoid\"),\n                Dense(units = 15, activation = \"sigmoid\"),\n                Dense(units = 1, activation = \"sigmoid\")])\n        \n        X = np.array([[.....]])\n        y = np.array([....])\n        \n        model.fit(X,y)\n        model.predict(x_new)\n        ```\n        \n    \n    //forward propagation from scratch in python\n    \n- Neural network implementation with python\n    \n    **forward prop in NumPy:**\n    \n    ![forward prop in single layer](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2019.png)\n    \n    forward prop in single layer\n    \n    ![NumPy implementation](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2020.png)\n    \n    NumPy implementation\n    \n- vectorization\n    \n    ![for loop vs vectorization](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2021.png)\n    \n    for loop vs vectorization\n    \n    ![matmul in numpy](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2022.png)\n    \n    matmul in numpy\n    \n    ![vectorized layer](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2023.png)\n    \n    vectorized layer"
  },
  {
    "objectID": "posts/ml-specialization2/index.html#week-1",
    "href": "posts/ml-specialization2/index.html#week-1",
    "title": "Advanced learning algorithm",
    "section": "",
    "text": "Neural network intuition\n\n\nbrief origin of neural networks\n\n\n\n\norigin of neural network\n\n\n\norigin of neural network\n\n\n\n\nwhy now?\n\n\n\nExample:\nsingle hidden layer\n\nThere is input layer with vector x which carries input features. these input are given to a hidden layer which outputs three activation values in the form of vector a, which is an input to output layer.\nThe neural network is smart enough to identify the features: affordability, awareness and perceived quality based on the input values.\n\n\n\nUntitled\n\n\n\n\n\ndemand prediction using neural network\n\n\ndemand prediction using neural network\nmultiple hidden layers - multilayer perceptron\n\n\n\nmultilayer perceptron\n\n\nFace recognition\n\n\n\nExample\n\n\n\n- Neural network model\n    \n    **How does a neural network layer work?**\n    \n    ![looking inside layer 1](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%206.png)\n    \n    looking inside layer 1\n    \n    ![inside layer 2](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%207.png)\n    \n    inside layer 2\n    \n    **Activation value for a given layer**\n    \n    ![notation for activation value/function](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%208.png)\n    \n    notation for activation value/function\n    \n    **forward propagation example:**\n    \n    computation goes from left to right. more hidden layers initially\n    \n    ![8*8 pixel image of handwritten digit](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%209.png)\n    \n    8*8 pixel image of handwritten digit\n    \n    compute a_1 and a_2\n    \n    ![compute activation value](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2010.png)\n    \n    compute activation value\n    \n    ![handwritten digit recognition with forward propagation](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2011.png)\n    \n    handwritten digit recognition with forward propagation\n    \n- TensorFlow implementation\n    \n    ![coffee roasting model](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2012.png)\n    \n    coffee roasting model\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2013.png)\n    \n    **tensorflow implementation of digit classification**\n    \n    ![digit classification inference](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2014.png)\n    \n    digit classification inference\n    \n    **data in tensorflow**\n    \n    ![2D numpy array ‚Üí matrix](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2015.png)\n    \n    2D numpy array ‚Üí matrix\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2016.png)\n    \n    ![tensorflow representation of data](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2017.png)\n    \n    tensorflow representation of data\n    \n    **Building a neural network architecture**\n    \n    ![neural network architecture using tensorflow](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2018.png)\n    \n    neural network architecture using tensorflow\n    \n    - **Here by convention the model is written as**\n        \n        ```python\n        model = Sequential([\n                Dense(units = 3, activation = \"sigmoid\"),\n                Dense(units = 1, activation = \"sigmoid\")])\n        ```\n        \n    - **Implementing NN architecture for digit classification using tensorflow**\n        \n        ```python\n        model = Sequential([\n                Dense(units = 25, activation = \"sigmoid\"),\n                Dense(units = 15, activation = \"sigmoid\"),\n                Dense(units = 1, activation = \"sigmoid\")])\n        \n        X = np.array([[.....]])\n        y = np.array([....])\n        \n        model.fit(X,y)\n        model.predict(x_new)\n        ```\n        \n    \n    //forward propagation from scratch in python\n    \n- Neural network implementation with python\n    \n    **forward prop in NumPy:**\n    \n    ![forward prop in single layer](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2019.png)\n    \n    forward prop in single layer\n    \n    ![NumPy implementation](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2020.png)\n    \n    NumPy implementation\n    \n- vectorization\n    \n    ![for loop vs vectorization](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2021.png)\n    \n    for loop vs vectorization\n    \n    ![matmul in numpy](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2022.png)\n    \n    matmul in numpy\n    \n    ![vectorized layer](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2023.png)\n    \n    vectorized layer"
  },
  {
    "objectID": "posts/ml-specialization2/index.html#week-2",
    "href": "posts/ml-specialization2/index.html#week-2",
    "title": "Advanced learning algorithm",
    "section": "Week 2",
    "text": "Week 2\n- Neural network training\n    \n    **how to build and train neural network in tensorFlow?**\n    \n    step 1: specify the model \n    \n    step 2: compile the model using specific loss function \n    \n    step 3: train the model\n    \n    ![steps in training a neural network in TensorFlow](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2024.png)\n    \n    steps in training a neural network in TensorFlow\n    \n    Model training: logistic regression vs neural network\n    \n    ![logistic regression vs neural network](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2025.png)\n    \n    logistic regression vs neural network\n    \n    **Step 1: Create the model**\n    \n    ![define model](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2026.png)\n    \n    define model\n    \n    **Step 2: Loss and cost function**\n    \n    ![choose lost and cost functions](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2027.png)\n    \n    choose lost and cost functions\n    \n    **Step 3: Gradient descent**\n    \n    ![Gradient descent](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2028.png)\n    \n    Gradient descent\n    \n- Activation function\n    \n    **Alternative to sigmoid function**\n    \n    ![Linear - Sigmoid - ReLU](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2029.png)\n    \n    Linear - Sigmoid - ReLU\n    \n    **How to choose activation function for neural network?**\n    \n    ***For output layer:***\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2030.png)\n    \n    ***For hidden layer:***\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2031.png)\n    \n    *here in sigmoid function there are two flat zone. using sigmoid function as activation function can result in more flat regions in cost function which results in slower gradient descent.*\n    \n    ***Summary:***\n    \n    For binary classification ‚Üí Sigmoid function\n    \n    For regression (+/-) ‚Üí Linear function\n    \n    For regression (0 or +) ‚Üí ReLU\n    \n    ***Code implementation:***\n    \n    ```python\n    from tf.keras.layers import Dense\n    model = Sequential([\n            Dense(units = 25, activation = 'relu'), #layer1\n            Dense(units = 15, activation = 'relu'), #layer2\n            Dense(units = 1, activation = 'sigmoid' or 'linear') #layer3\n    ])\n    ```\n    \n    **Why do we need activation functions?**\n    \n    if we only use linear function for each layer of neural network then the neural network is equivalent to a linear regression \n    \n    ![neural network ‚Üí linear regression](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2032.png)\n    \n    neural network ‚Üí linear regression\n    \n    ***Don‚Äôt use linear activation in hidden layers:***\n    \n    ![neural network ‚Üí logistic regression](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2033.png)\n    \n    neural network ‚Üí logistic regression\n    \n- Multiclass classification\n    \n    **how does multiclass classification problem look like?**\n    \n    ![multiclass example](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2034.png)\n    \n    multiclass example\n    \n    **what is Softmax?**\n    \n    generalization of logistic regression algorithm\n    \n    ![softmax regression mathematical form](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2035.png)\n    \n    softmax regression mathematical form\n    \n    ***cost function for softmax:***\n    \n    ![cost for softmax](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2036.png)\n    \n    cost for softmax\n    \n    **Neural network with softmax output:**\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2037.png)\n    \n    ***basic implementation with tensorflow:***\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2038.png)\n    \n    **Improved implementation of softmax:**\n    \n    this method is suitable to account for minimizing numerical roundoff errors\n    \n    ***what‚Äôs numerical roundoff error?***\n    \n    ![numerical roundoff error](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2039.png)\n    \n    numerical roundoff error\n    \n    ***for logistic regression:***\n    \n    ![minimize numerical roundoff error for logistic regression](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2040.png)\n    \n    minimize numerical roundoff error for logistic regression\n    \n    ***code***:\n    \n    ![numerically accurate logistic regression](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2041.png)\n    \n    numerically accurate logistic regression\n    \n    ***for softmax:***\n    \n    ![improved implementation of softmax](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2042.png)\n    \n    improved implementation of softmax\n    \n    ![steps for numerically accurate softmax output](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2043.png)\n    \n    steps for numerically accurate softmax output\n    \n    **Multi-label classification:**\n    \n    ![multi-label classification example](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2044.png)\n    \n    multi-label classification example\n    \n    ![Untitled](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2045.png)\n    \n- Additional neural network concept\n    \n    **Adam algorithm: GD alternative**\n    \n    ![Adam algorithm mathematical form](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2046.png)\n    \n    Adam algorithm mathematical form\n    \n    ![Adam Algo Intuition](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2047.png)\n    \n    Adam Algo Intuition\n    \n    ![Code for Adam](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2048.png)\n    \n    Code for Adam\n    \n    **Convolutional layer:**\n    \n    ![dense layer alternative](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2049.png)\n    \n    dense layer alternative\n    \n    **CNN:**\n    \n    ![CNN](Advanced%20learning%20algorithm%20@November%2024,%202023%20f830f807954140d0898eb9ae02adfab7/Untitled%2050.png)\n    \n    CNN\n    \n- Back propagation\n\nWeek 3\n\nAdvice for applying machine learning\nBias and Variance\nMachine learning development process\nSkewed dataset\n\nWeek 4\n\nDecision trees\ndecision tree learning\ntree ensembles"
  }
]
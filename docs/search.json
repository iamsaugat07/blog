[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "$hello_world!",
    "section": "",
    "text": "Predicting heart disease using machine learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMy curriculum for learning machine learning in 2023\n\n\n\n\n\n\n\nMachine learning & AI\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSaugat\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nSaugat\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mlcurriculum/index.html",
    "href": "posts/mlcurriculum/index.html",
    "title": "My curriculum for learning machine learning in 2023",
    "section": "",
    "text": "(env) saugat: brain/cerebral cortex/machine learning-AI\nDisclaimer : This post is not written or edited by chatgpt or any AI tool üòÅ\nPheww! The world has seen the tremendous impact of large language models(LLMs) like gpt-3.5 or gpt 4 in recent times. There has been new emerging fields like prompt engineering, LLMOps which was never seen before. The foundation for all these emerging technology has always been machine learning and software engineering. To be more specific Natural language processing and its applications augmented by different software infrastructure and data.\nThe topic of LLMs can be a different blog post. Lets get back to our main topic.\nHere is little background: I am mechanical engineering graduate of 2022 from Institute of engineering(IOE), TU Nepal. Having worked as design and CFD engineer with couple of startups in automotive industry in Nepal for almost 2 years, (I even started racing team in my college: Team Shireto) I decided to switch to world of machine learning and AI in march of 2023. Learning new things is always exciting for me. One of the habits or instinct that you need to learn when you come from engineering background is learning the hard concepts quickly and being able to create something valuable out of it aka solving an engineering problem. The vast application of machine learning in different industry ranging from automotive to finance to healthcare and so many more is one of the reason behind my focus on machine learning to solve engineering problems. You are not limited to a certain domain when it comes to applying machine learning in contrast with the field I was previously focusing on.\nSo how did I get started with learning machine learning in 2023?\nI went through internet and found tons of free resources. As I couldn‚Äôt afford to pay for most of the online courses these free resources are like diamonds for me. Damn! we live in a world where we can learn anything for free. All those readings boiled down to this curriculum which I created for myself to become a self taught ML engineer.\nOne of the main challenge to get started with machine learning was programming. Maths was not a big problem as my engineering degree covered all the required maths foundation for machine learning.\nLet me list down courses in a format that I am following during this journey of becoming an ML practitioner.\n\nComplete Machine Learning & Data Science Bootcamp 2023 by ZTM\n\nWhy this course?\nHmm‚Ä¶ So this is the only course that I paid for. Learning something new comes with lot of resistance in the beginning. I have fell into enroll and never see the course again loop so many times before when I wanted to learn programming, therefore I decided to pay for this course so I would push myself to learn consistently and finish the course start to end, which I eventually did in almost 2 months. This course help me build my foundation on programming as it focuses on run the code approach and project based approach to learn programming.\nI learnt about python as required for me to get started for using specialized python based packages for ML & data science like Numpy, pandas, matplotlib and scikitlearn. During the course I build three projects which I will write about in my future blog. In summary, I worked on heart disease classification project, bulldozer auction price prediction project and dog breed identification project(using tensorflow). To be honest most of the things went over my head while working on these projects. But I worked hard and made sure I finish the course no matter what. I watched videos on loop, searched internet, interacted with community built around the course and asked ChatGPT to digest the content and ingest the concept into my memory.\n\nCS50 Harvard‚Äôs Introduction to computer science\n\nTo understand more about computer science and programming in general, I joined this course on free version. I haven‚Äôt finished it yet. As the course problem set are quite challenging and takes time. I am huge fan of interleaving, which basically means interleaving between concepts in different domains or subject to learn proactively. So during my boot camp, I interleaved between courses to learn effectively. But how? I will write a another blog post on this üòÇ.\nThis course focuses on fundamentals of computer science from binary to memory, from arrays to algorithms and different programming language like python, C, HTML, CSS, Javascript & SQL. Hopefully I will be able to finish this course very soon.\n\nMachine learning specialization by Deeplearning.ai\n\nThis has to be the best among all the course in my list. Wait not only on my list but whole courses on ML or engineering in the whole world. I became a huge fan of Andrew when I joined this course. The way he explained the concepts from basics and visualize all the fundamental concepts like gradient descent, cost function, evaluation metrics, neural network, Random forest from scratch is extraordinary. The code implementation of ML algorithms from scratch was the best part of this course. Talking about interleaving, while working on my classification problem during boot camp I joined Andrew‚Äôs course to understand the underlying concepts by watching the videos on logistic regression and classification. Still I haven‚Äôt finished all the courses in this specialization. But whatever I have learned so far has been so fruitful to build my intuition for machine learning.\n\nFast.ai deep learning for coders\n\nThis is the toughest course among all for me. Part of the reason is the book that I am following alongside the course. The book and the course implements top to bottom approach when it comes to learning, which is unusual and unique when comparing to all other courses. What does that mean? Basically top to bottom approach is to start learning by seeing the application of the code and getting your model running before actually going deep inside the theory and the behind the scenes of the algorithms, which is completely opposite to way it is taught the machine learning specialization. There are two part in this course. One is deep learning foundations another is deep learning foundations to stable diffusion.\nI am currently on part one where I have finished first four lesson where I learned about getting started with neural network, getting your model to production aka building ml based web app and basic NLP.\n\nFull Stack deep learning\n\nThis has to be bring it all together course where you will not learn how to write your own model or creating training and validation sets but you will learn how to build products powered by ML. One of the main concept or eye opener for me during this course was that the training of models is starting to become commoditized. I learnt about different tools like huggingface that you can use to deploy state-of-the-art models in few lines of code. This is the most exciting course in the list where you focus on learning ML product engineering.\n\n\nChatGPT\n\nWell ChatGPT is more off a tool to learn and search about some topics and get a quick read rather than searching the internet. During my interaction with ChatGPT, I am focusing to write good prompts so that I can get what I want from the GPT3.5 model. It helps with quick example of code and ask MCQ related to the explanation. \n\n\n\n\nThe missing semester of your CS education\n\nThis course teaches about computer ecosystem literacy aka the tools you need to use during your career as a software engineer or developer. It has all the necessary topic that you will encounter during the journey of becoming a developer and helps with saving time using the tools and concepts like shell, command line interface, data wrangling, version control, debugging & profiling and more.\nAlongside following the course I think it is also important to look around the community and follow top ML practitioner who share their knowledge and experiences through blogs, podcast and tweets.\nHere is the list of ML practitioner who have influenced me a lot (list of practitioner who are actively writing, tweeting & sharing their learnings)\n\nAndrej Karpathy\nChip Huyen\nEmil Walner\nJeremy Howard\nJonas Degrave\n\nAnd of course Andrew Ng is my favorite üëç\nI feel like each subtopic in this blog can be converted into another blog post and that blog post‚Äôs subtopic into another in form of nested blogs. In future, I will come up with more blogs sharing about my learnings, practices and mistakes along the way. I would love to hear from other people who might read my blog and share their feedback on how can I learn better and get better in Machine learning & AI. Now, alongside these course I will start building my own personal projects, contributing to opensource and eventually developing & deploying ML(or LLMsüòâ) powered products.\nEnding this blog with a tweet from Andrej."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html",
    "title": "Predicting heart disease using machine learning",
    "section": "",
    "text": "---\ntitle: \"Heart disease prediction\"\nauthor: \"Saugat\"\ndate: \"6/3/2021\"\nformat: \n  html:\n    code-fold: true\njupyter: python3\n---\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether someone has heart disease based on their medical attributes\nwe‚Äôre going to take following approach: 1. Problem definition 2. Data 3. Evaluation 4. Features 5. Modeling 6. Experimentation"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#problem-definition",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#problem-definition",
    "title": "Predicting heart disease using machine learning",
    "section": "1. Problem definition",
    "text": "1. Problem definition\nIn a statement, &gt; Given clinical parameters about a patient, can we predict a patient has heart disease."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#data",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#data",
    "title": "Predicting heart disease using machine learning",
    "section": "2. Data",
    "text": "2. Data\nThe original data came from the Cleavland data from UCI machine learning repository - https://archive.ics.uci.edu/ml/datasets/heart+disease\nOn kaggle - https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluation",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluation",
    "title": "Predicting heart disease using machine learning",
    "section": "3. Evaluation",
    "text": "3. Evaluation\n\nIf we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we‚Äôll pursue the project."
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#features",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#features",
    "title": "Predicting heart disease using machine learning",
    "section": "4. Features",
    "text": "4. Features\nThis is where you‚Äôll get different information about each of the features in your data\ncreate data dictionary\n\nage - age in years\nsex - (1 = male; 0 = female)\ncp - chest pain type\n\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related) *3: Asymptomatic: chest pain not showing signs of disease\n\ntrestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\nanything above 130-140 is typically cause for concern\n\nchol - serum cholestoral in mg/dl\n\nserum = LDL + HDL + .2 * triglycerides\nabove 200 is cause for concern\n\nfbs - (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n\n‚Äò&gt;126‚Äô mg/dL signals diabetes\n\nrestecg - resting electrocardiographic results\n\n0: Nothing to note\n1: ST-T Wave abnormality\n\ncan range from mild symptoms to severe problems\nsignals non-normal heart beat\n\n2: Possible or definite left ventricular hypertrophy\n\nEnlarged heart‚Äôs main pumping chamber\n\n\nthalach - maximum heart rate achieved\nexang - exercise induced angina (1 = yes; 0 = no)\noldpeak - ST depression induced by exercise relative to rest\n\nlooks at stress of heart during excercise\nunhealthy heart will stress more\n\nslope - the slope of the peak exercise ST segment\n\n0: Upsloping: better heart rate with excercise (uncommon)\n1: Flatsloping: minimal change (typical healthy heart)\n2: Downslopins: signs of unhealthy heart\n\nca - number of major vessels (0-3) colored by flourosopy\n\ncolored vessel means the doctor can see the blood passing through\n\nthal - thalium stress result\n\n1,3: normal\n6: fixed defect: used to be defect but ok now\n7: reversable defect: no proper blood movement when excercising\n\ntarget - have disease or not (1=yes, 0=no) (= the predicted attribute)"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#preparing-the-tools",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#preparing-the-tools",
    "title": "Predicting heart disease using machine learning",
    "section": "Preparing the tools",
    "text": "Preparing the tools\nwe‚Äôre going to use pandas, matplotlib and Numpy for data analysis and manipulation\n\n# import all the tools\n\n# Regular EDA(exploratory data analysis) and plotting library\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#we want our plots to appear inside the notebook\n%matplotlib inline \n\n# models from scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model evaluations \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import RocCurveDisplay"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#load-data",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#load-data",
    "title": "Predicting heart disease using machine learning",
    "section": "Load data",
    "text": "Load data\n\ndf = pd.read_csv(\"data/heart-disease (1).csv\")\ndf.shape\n\n(303, 14)"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#data-exploration-exploratory-data-analysis-or-eda",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#data-exploration-exploratory-data-analysis-or-eda",
    "title": "Predicting heart disease using machine learning",
    "section": "Data Exploration ( Exploratory data analysis or EDA)",
    "text": "Data Exploration ( Exploratory data analysis or EDA)\nThe goal here is to find out more about the data and become a subject matter export on the dataset you‚Äôre working with\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat‚Äôs missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?\n\n\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\n#lets find out how many of each class exists\ndf.target.value_counts()\n\n1    165\n0    138\nName: target, dtype: int64\n\n\n\ndf.target.value_counts().plot(kind = \"bar\", color = [\"salmon\",\"lightblue\"])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trestbps  303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalach   303 non-null    int64  \n 8   exang     303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slope     303 non-null    int64  \n 11  ca        303 non-null    int64  \n 12  thal      303 non-null    int64  \n 13  target    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n\n\n\ndf.isna().sum()\n\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ntarget      0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\ncount\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n303.000000\n\n\nmean\n54.366337\n0.683168\n0.966997\n131.623762\n246.264026\n0.148515\n0.528053\n149.646865\n0.326733\n1.039604\n1.399340\n0.729373\n2.313531\n0.544554\n\n\nstd\n9.082101\n0.466011\n1.032052\n17.538143\n51.830751\n0.356198\n0.525860\n22.905161\n0.469794\n1.161075\n0.616226\n1.022606\n0.612277\n0.498835\n\n\nmin\n29.000000\n0.000000\n0.000000\n94.000000\n126.000000\n0.000000\n0.000000\n71.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n47.500000\n0.000000\n0.000000\n120.000000\n211.000000\n0.000000\n0.000000\n133.500000\n0.000000\n0.000000\n1.000000\n0.000000\n2.000000\n0.000000\n\n\n50%\n55.000000\n1.000000\n1.000000\n130.000000\n240.000000\n0.000000\n1.000000\n153.000000\n0.000000\n0.800000\n1.000000\n0.000000\n2.000000\n1.000000\n\n\n75%\n61.000000\n1.000000\n2.000000\n140.000000\n274.500000\n0.000000\n1.000000\n166.000000\n1.000000\n1.600000\n2.000000\n1.000000\n3.000000\n1.000000\n\n\nmax\n77.000000\n1.000000\n3.000000\n200.000000\n564.000000\n1.000000\n2.000000\n202.000000\n1.000000\n6.200000\n2.000000\n4.000000\n3.000000\n1.000000\n\n\n\n\n\n\n\n\nHeart disease frequency according to sex\n\ndf.sex.value_counts()\ndf.target.value_counts()\n\n1    165\n0    138\nName: target, dtype: int64\n\n\n\n pd.crosstab(df.target, df.sex)\n\n\n\n\n\n\n\nsex\n0\n1\n\n\ntarget\n\n\n\n\n\n\n0\n24\n114\n\n\n1\n72\n93\n\n\n\n\n\n\n\n\npd.crosstab(df.target,df.sex).plot(kind = \"bar\",\n                                  color =[\"salmon\",\"lightblue\"],\n                                  figsize =(10,6))\nplt.title(\"Heart Disease Frequency for sex\")\nplt.xlabel(\"0 = No disease, 1 = disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"]);\nplt.xticks(rotation=0)\n\n(array([0, 1]), [Text(0, 0, '0'), Text(1, 0, '1')])\n\n\n\n\n\n\npd.crosstab(df.target, df.age)\n\n\n\n\n\n\n\nage\n29\n34\n35\n37\n38\n39\n40\n41\n42\n43\n...\n65\n66\n67\n68\n69\n70\n71\n74\n76\n77\n\n\ntarget\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n2\n0\n1\n1\n2\n1\n1\n3\n...\n4\n3\n6\n2\n1\n3\n0\n0\n0\n1\n\n\n1\n1\n2\n2\n2\n2\n3\n1\n9\n7\n5\n...\n4\n4\n3\n2\n2\n1\n3\n1\n1\n0\n\n\n\n\n2 rows √ó 41 columns\n\n\n\n\npd.crosstab(df.target,df.exang)\n\n\n\n\n\n\n\nexang\n0\n1\n\n\ntarget\n\n\n\n\n\n\n0\n62\n76\n\n\n1\n142\n23\n\n\n\n\n\n\n\n\n\nAge vs max heart rate for heart disease\n\n# create another figure\nplt.figure(figsize=(10,6))\n\n#scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1],\n           c=\"salmon\")\n\n#scatter with neg example\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           c=\"lightblue\");\n\nplt.title(\"Heart disease in function of age and max heart rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max heart rate\")\nplt.legend([\"Disease\",\"No disease\"]);\n\n\n\n\n\n#check the distribution of the age column with a histogram\ndf.age.plot.hist();\n\n\n\n\n\n\nHeart disease frequency per chest pain type\n\ncp - chest pain type\n\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related)\n\n\n\npd.crosstab(df.cp,df.target)\n\n\n\n\n\n\n\ntarget\n0\n1\n\n\ncp\n\n\n\n\n\n\n0\n104\n39\n\n\n1\n9\n41\n\n\n2\n18\n69\n\n\n3\n7\n16\n\n\n\n\n\n\n\n\n#make the crosstab more visual\npd.crosstab(df.cp,df.target).plot(kind=\"bar\",\n                                 figsize =(10,6),\n                                 color = [\"salmon\",\"lightblue\"])\nplt.title(\"Heart disease frequency per chest pain type\")\nplt.xlabel(\"Chest pain type\")\nplt.ylabel(\"amount\")\nplt.legend([\"No disease\",\"Disease\"])\nplt.xticks(rotation=0);\n\n\n\n\n\n# make a correlation matrix\ndf.corr()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\nage\n1.000000\n-0.098447\n-0.068653\n0.279351\n0.213678\n0.121308\n-0.116211\n-0.398522\n0.096801\n0.210013\n-0.168814\n0.276326\n0.068001\n-0.225439\n\n\nsex\n-0.098447\n1.000000\n-0.049353\n-0.056769\n-0.197912\n0.045032\n-0.058196\n-0.044020\n0.141664\n0.096093\n-0.030711\n0.118261\n0.210041\n-0.280937\n\n\ncp\n-0.068653\n-0.049353\n1.000000\n0.047608\n-0.076904\n0.094444\n0.044421\n0.295762\n-0.394280\n-0.149230\n0.119717\n-0.181053\n-0.161736\n0.433798\n\n\ntrestbps\n0.279351\n-0.056769\n0.047608\n1.000000\n0.123174\n0.177531\n-0.114103\n-0.046698\n0.067616\n0.193216\n-0.121475\n0.101389\n0.062210\n-0.144931\n\n\nchol\n0.213678\n-0.197912\n-0.076904\n0.123174\n1.000000\n0.013294\n-0.151040\n-0.009940\n0.067023\n0.053952\n-0.004038\n0.070511\n0.098803\n-0.085239\n\n\nfbs\n0.121308\n0.045032\n0.094444\n0.177531\n0.013294\n1.000000\n-0.084189\n-0.008567\n0.025665\n0.005747\n-0.059894\n0.137979\n-0.032019\n-0.028046\n\n\nrestecg\n-0.116211\n-0.058196\n0.044421\n-0.114103\n-0.151040\n-0.084189\n1.000000\n0.044123\n-0.070733\n-0.058770\n0.093045\n-0.072042\n-0.011981\n0.137230\n\n\nthalach\n-0.398522\n-0.044020\n0.295762\n-0.046698\n-0.009940\n-0.008567\n0.044123\n1.000000\n-0.378812\n-0.344187\n0.386784\n-0.213177\n-0.096439\n0.421741\n\n\nexang\n0.096801\n0.141664\n-0.394280\n0.067616\n0.067023\n0.025665\n-0.070733\n-0.378812\n1.000000\n0.288223\n-0.257748\n0.115739\n0.206754\n-0.436757\n\n\noldpeak\n0.210013\n0.096093\n-0.149230\n0.193216\n0.053952\n0.005747\n-0.058770\n-0.344187\n0.288223\n1.000000\n-0.577537\n0.222682\n0.210244\n-0.430696\n\n\nslope\n-0.168814\n-0.030711\n0.119717\n-0.121475\n-0.004038\n-0.059894\n0.093045\n0.386784\n-0.257748\n-0.577537\n1.000000\n-0.080155\n-0.104764\n0.345877\n\n\nca\n0.276326\n0.118261\n-0.181053\n0.101389\n0.070511\n0.137979\n-0.072042\n-0.213177\n0.115739\n0.222682\n-0.080155\n1.000000\n0.151832\n-0.391724\n\n\nthal\n0.068001\n0.210041\n-0.161736\n0.062210\n0.098803\n-0.032019\n-0.011981\n-0.096439\n0.206754\n0.210244\n-0.104764\n0.151832\n1.000000\n-0.344029\n\n\ntarget\n-0.225439\n-0.280937\n0.433798\n-0.144931\n-0.085239\n-0.028046\n0.137230\n0.421741\n-0.436757\n-0.430696\n0.345877\n-0.391724\n-0.344029\n1.000000\n\n\n\n\n\n\n\n\n# lets make our correlation matrix visual\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                 annot = True,\n                 linewidths = 0.5,\n                 fmt = \".2f\",\n                 cmap = \"YlGnBu\");"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#modelling",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#modelling",
    "title": "Predicting heart disease using machine learning",
    "section": "5. Modelling",
    "text": "5. Modelling\n\n# split into X & y\nX = df.drop(\"target\", axis =1)\ny = df[\"target\"]\n\n\nX\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n298\n57\n0\n0\n140\n241\n0\n1\n123\n1\n0.2\n1\n0\n3\n\n\n299\n45\n1\n3\n110\n264\n0\n1\n132\n0\n1.2\n1\n0\n3\n\n\n300\n68\n1\n0\n144\n193\n1\n1\n141\n0\n3.4\n1\n2\n3\n\n\n301\n57\n1\n0\n130\n131\n0\n1\n115\n1\n1.2\n1\n1\n3\n\n\n302\n57\n0\n1\n130\n236\n0\n0\n174\n0\n0.0\n1\n1\n2\n\n\n\n\n303 rows √ó 13 columns\n\n\n\n\ny\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n298    0\n299    0\n300    0\n301    0\n302    0\nName: target, Length: 303, dtype: int64\n\n\n\n# split data into train and test set\nnp.random.seed(42)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2)\n\n\nX_train\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n132\n42\n1\n1\n120\n295\n0\n1\n162\n0\n0.0\n2\n0\n2\n\n\n202\n58\n1\n0\n150\n270\n0\n0\n111\n1\n0.8\n2\n0\n3\n\n\n196\n46\n1\n2\n150\n231\n0\n1\n147\n0\n3.6\n1\n0\n2\n\n\n75\n55\n0\n1\n135\n250\n0\n0\n161\n0\n1.4\n1\n0\n2\n\n\n176\n60\n1\n0\n117\n230\n1\n1\n160\n1\n1.4\n2\n2\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n188\n50\n1\n2\n140\n233\n0\n1\n163\n0\n0.6\n1\n1\n3\n\n\n71\n51\n1\n2\n94\n227\n0\n1\n154\n1\n0.0\n2\n1\n3\n\n\n106\n69\n1\n3\n160\n234\n1\n0\n131\n0\n0.1\n1\n1\n2\n\n\n270\n46\n1\n0\n120\n249\n0\n0\n144\n0\n0.8\n2\n0\n3\n\n\n102\n63\n0\n1\n140\n195\n0\n1\n179\n0\n0.0\n2\n2\n2\n\n\n\n\n242 rows √ó 13 columns\n\n\n\nNow we‚Äôve got our data split into train and test sets, its time to use machine learning\nWe‚Äôre going to try 3 different machine learning model 1. Logistic regression 2. K-Nearest Neighbours classifier 3. Random forest classifier\n\n# put models in a dictionary\nmodels = {\"logistics regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# creare a function to fit and score models \ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different scikit-learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test label\n    \"\"\"\n    #set random seed\n    np.random.seed(42)\n    \n    #set empty dict to store model scores\n    model_scores = {}\n    \n    # looping through model dict\n    for name, model in models.items():\n        # fit the model\n        model.fit(X_train, y_train)\n        #evaluate the model\n        model_scores [name] = model.score(X_test, y_test)\n    return model_scores\n\n\nmodel_scores = fit_and_score(models, X_train,X_test,y_train,y_test)\nmodel_scores\n\nC:\\Users\\Team Shireto\\Desktop\\sample_project_1\\env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n{'logistics regression': 0.8852459016393442,\n 'KNN': 0.6885245901639344,\n 'Random Forest': 0.8360655737704918}\n\n\n\nModel comparison\n\n\nmodel_compare = pd.DataFrame(model_scores, index = [\"accuracy\"])\nmodel_compare.T.plot.bar();\n\n\n\n\nNow we‚Äôve got a baseline model and we know model‚Äôs first predicction aren‚Äôt always final\nlets look at the following: * Hyperparameter tuning * Feature importance * Confusion matrix * Cross-validation * Precision * Recall * F1 score * Classification report * ROC vurve * Area under the curve (AUC)\n\n\nHyperparameter tuning\n\n#let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# create a list of different values for n-neighbours \nneighbors = range(1,21)\n\n# setup KNN instance\nknn = KNeighborsClassifier()\n\n# loop for different value of n-neighbours\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    #fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    #update the training score \n    train_scores.append(knn.score(X_train,y_train))\n    \n    #update the test score \n    test_scores.append(knn.score(X_test, y_test))\n\n\ntrain_scores\n\n[1.0,\n 0.8099173553719008,\n 0.7727272727272727,\n 0.743801652892562,\n 0.7603305785123967,\n 0.7520661157024794,\n 0.743801652892562,\n 0.7231404958677686,\n 0.71900826446281,\n 0.6942148760330579,\n 0.7272727272727273,\n 0.6983471074380165,\n 0.6900826446280992,\n 0.6942148760330579,\n 0.6859504132231405,\n 0.6735537190082644,\n 0.6859504132231405,\n 0.6652892561983471,\n 0.6818181818181818,\n 0.6694214876033058]\n\n\n\nplt.plot(neighbors, train_scores, label = \"train score\")\nplt.plot(neighbors, test_scores, label = \"test score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"number of neighbors\")\nplt.ylabel(\"model score\")\nplt.legend()\n\nprint(f\"The best accuracy of the KNN model is {max(test_scores)*100:.2f} %\")\n\nThe best accuracy of the KNN model is 75.41 %"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-tuning-with-randomizedsearchcv",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-tuning-with-randomizedsearchcv",
    "title": "Predicting heart disease using machine learning",
    "section": "hyperparameter tuning with randomizedsearchCV",
    "text": "hyperparameter tuning with randomizedsearchCV\nwe‚Äôre going to tune: * LogisticRegression() * RandomForestClassifier()\nusing randomizedsearchCV\n\n# create hyperparamter grid for logisticRegression\nlog_reg_grid = {\"C\" : np.logspace(-4,4,20),\n               \"solver\" : [\"liblinear\"]}\n\n#create hyperparameter grid for RandomSearchCV\nrf_grid = {\"n_estimators\" : np.arange(10,1000,50),\n          \"max_depth\" : [None, 3,5,10],\n          \"min_samples_split\" : np.arange(2,20,2),\n          \"min_samples_leaf\" : np.arange(1,20,2)}\n\nlets use RandomizedSearchCV\n\n# tune logisticRegression \n  \nnp.random.seed(42)\n\n# set up random hyperparameter for logisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv =5,\n                               n_iter = 20,\n                               verbose = True)\n\n#fit the model\nrs_log_reg.fit(X_train, y_train)\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\nRandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=20,\n                   param_distributions={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                                        'solver': ['liblinear']},\n                   verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=20,\n                   param_distributions={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                                        'solver': ['liblinear']},\n                   verbose=True)estimator: LogisticRegressionLogisticRegression()LogisticRegressionLogisticRegression()\n\n\n\nrs_log_reg.best_params_\n\n{'solver': 'liblinear', 'C': 0.23357214690901212}\n\n\n\nrs_log_reg.score(X_test, y_test)\n\n0.8852459016393442\n\n\n\n# tune RandomForestClassifier\n  \nnp.random.seed(42)\n\n# set up random hyperparameter for randomforestclassifier\nrs_rf= RandomizedSearchCV(RandomForestClassifier(),\n                               param_distributions=rf_grid,\n                               cv =5,\n                               n_iter = 20,\n                               verbose = True)\n\n#fit the model\nrs_rf.fit(X_train, y_train)\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=20,\n                   param_distributions={'max_depth': [None, 3, 5, 10],\n                                        'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n                                        'n_estimators': array([ 10,  60, 110, 160, 210, 260, 310, 360, 410, 460, 510, 560, 610,\n       660, 710, 760, 810, 860, 910, 960])},\n                   verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=20,\n                   param_distributions={'max_depth': [None, 3, 5, 10],\n                                        'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n                                        'n_estimators': array([ 10,  60, 110, 160, 210, 260, 310, 360, 410, 460, 510, 560, 610,\n       660, 710, 760, 810, 860, 910, 960])},\n                   verbose=True)estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\n\n\nrs_rf.best_params_\n\n{'n_estimators': 210,\n 'min_samples_split': 4,\n 'min_samples_leaf': 19,\n 'max_depth': 3}\n\n\n\nrs_rf.score(X_test,y_test)\n\n0.8688524590163934"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-turning-using-gridsearchcv",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#hyperparameter-turning-using-gridsearchcv",
    "title": "Predicting heart disease using machine learning",
    "section": "Hyperparameter turning using GridSearchCV",
    "text": "Hyperparameter turning using GridSearchCV\n\n# different hyperparameter for our logisticRegression Model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n                \"solver\":[\"liblinear\"]\n               }\n\nnp.random.seed(42)\n\n#setup grid hyperparameter search for logisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv =5,\n                         verbose = True)\n\n#fit our model\ngs_log_reg.fit(X_train,y_train);\n\nFitting 5 folds for each of 30 candidates, totalling 150 fits\n\n\n\n# check best hyperparameter\ngs_log_reg.best_params_\n\n{'C': 0.20433597178569418, 'solver': 'liblinear'}\n\n\n\ngs_log_reg.score(X_test,y_test)\n\n0.8852459016393442"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluating-our-tuned-machine-learning-classfier-beyond-accuracy",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#evaluating-our-tuned-machine-learning-classfier-beyond-accuracy",
    "title": "Predicting heart disease using machine learning",
    "section": "Evaluating our tuned machine learning classfier, beyond accuracy",
    "text": "Evaluating our tuned machine learning classfier, beyond accuracy\n\nROC curve and AUC score\nConfusion matrix\nClassification report\nPrecision\nRecall\nF1-score\n\nTo make comparisons and evaluate our trained model, first we need to make predictions\n\n# make prediction with tuned model\ny_preds = gs_log_reg.predict(X_test)\n\n\ny_preds\n\narray([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)\n\n\n\n# plot ROC curve and calculate AUC score\nRocCurveDisplay.from_estimator(gs_log_reg,X_test,y_test);\n\n\n\n\n\n# Confusion matrix \nprint(confusion_matrix(y_test,y_preds))\n\n[[25  4]\n [ 3 29]]\n\n\n\nsns.set(font_scale= 1.5)\n\ndef plot_conf_mat(y_test,y_preds):\n    \"\"\"\n    PLots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot = True,\n                    cbar = False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test,y_preds)\n\n\n\n\nlets get classification report as well as cross-validated precision, recall and f1 score.\n\nprint(classification_report(y_test,y_preds))\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.86      0.88        29\n           1       0.88      0.91      0.89        32\n\n    accuracy                           0.89        61\n   macro avg       0.89      0.88      0.88        61\nweighted avg       0.89      0.89      0.89        61\n\n\n\n\nCalculate evaluation metrics using cross validation\n\n# check best hyperparameters \ngs_log_reg.best_params_\n\n{'C': 0.20433597178569418, 'solver': 'liblinear'}\n\n\n\n# create a new classifier with best params \nclf = LogisticRegression(C=0.20433597178569418,\n                        solver = \"liblinear\")\n\n\n\n# cross validated accuracy\ncv_acc = cross_val_score(clf, X, y, cv = 5, scoring= \"accuracy\")\ncv_acc\n\narray([0.81967213, 0.90163934, 0.86885246, 0.88333333, 0.75      ])\n\n\n\ncv_acc = np.mean(cv_acc)\ncv_acc\n\n0.8446994535519124\n\n\n\n# cross validated precision\ncv_precision =cross_val_score(clf, X, y, cv = 5, scoring= \"precision\")\n\ncv_precision = np.mean(cv_precision)\ncv_precision\n\n0.8207936507936507\n\n\n\n# cross validated recall \ncv_recall = cross_val_score(clf, X, y, cv = 5, scoring= \"recall\")\n\ncv_recall = np.mean(cv_recall)\ncv_recall\n\n0.9212121212121213\n\n\n\n# cross validated f1-score\ncv_f1 = cross_val_score(clf, X, y, cv = 5, scoring= \"precision\")\n\ncv_f1 = np.mean(cv_f1)\ncv_f1\n\n0.8207936507936507\n\n\n\n# visualize cross validated metrics \ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                          \"Precision\" : cv_precision,\n                          \"recall\" : cv_recall,\n                          \"f1-score\" : cv_f1},\n                          index = [0])\ncv_metrics.T.plot.bar(title = \"Cross validated evaluation metrics\",legend = 0);\n\n\n\n\n\n\nfeature importance\nWhich feature contributed the most to the outcomes of the model and how did they contribute\n\n# fit an instance of logisticregressionabs\nclf.fit(X_train,y_train);\n\n\n# check coef_\nclf.coef_\n\narray([[ 0.00316728, -0.86044674,  0.66067031, -0.01156993, -0.00166375,\n         0.04386101,  0.31275865,  0.02459362, -0.60413094, -0.56862789,\n         0.45051632, -0.63609908, -0.67663375]])\n\n\n\n# match coef's of features to columns \nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict\n\n{'age': 0.0031672830780218957,\n 'sex': -0.8604467440762573,\n 'cp': 0.6606703120090932,\n 'trestbps': -0.011569932037408597,\n 'chol': -0.00166374523064295,\n 'fbs': 0.043861009724542044,\n 'restecg': 0.3127586507840532,\n 'thalach': 0.024593615555173243,\n 'exang': -0.6041309439103262,\n 'oldpeak': -0.5686278914396258,\n 'slope': 0.4505163222528207,\n 'ca': -0.6360990763634887,\n 'thal': -0.6766337475895309}\n\n\n\n# visualize feature importance\nfeature_df = pd.DataFrame(feature_dict,index=[0])\nfeature_df.T.plot.bar(title = \"Feature Importance\", legend = 0\n                     );"
  },
  {
    "objectID": "posts/my first ml model/end-to-end-heart-disease-classification.html#experimentation",
    "href": "posts/my first ml model/end-to-end-heart-disease-classification.html#experimentation",
    "title": "Predicting heart disease using machine learning",
    "section": "6. Experimentation",
    "text": "6. Experimentation\nIf the evaluation metrics is not met:\n\nCould you collect more data?\nCould you try better model? like catboost or XGBoost?\nCould we improve the current model ? (beyond what we have done so far)\nif you meet your evaluation metrics, how would you share it with others?"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nLarge Language Model have generated a shift in our daily life. We can already see the impact of LLMs based appilcation like ChatGPT in our daily lives.\nThe fundamental working principle behind such LLMs has been deep learning. I am grateful enough to live in this period where we can leverage such powerful models to do anything that involves natural language.\nAnd this is exciting as I begin to learn more about language models, machine learning, AI and whatever the heck that comes out in the future. I feel like this is the right time to get into machine learning and AI for anyone literally.\n source: 5 things you should know about AI.(Cambridge consultant, 2017)\nHere I will share my journey of becoming an AI developer, bringing AI and Machine learning based product to production in real world."
  }
]